{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Factorization Machine 因子分解机","text":"FM主要是给考虑关联特征的线性模型的求解提供一种新的思路。 一般的线性模型可以表示为：$$y(x)=w_0+\\sum_\\limits{i=1}^{n}{w_i}x_i$$但是上述模型没有考虑特征间的关联，为表示关联特征对$y$的影响，考虑多项式模型，以$x_iy_i$表示两特征的组合，有如下二阶多项式模型：$$y=w_0+\\sum_\\limits{i=1}^{n}{w_ix_i}+\\sum_limits{i=1}^{n-1}{\\sum_\\limits{j=i+1}^{n}{w_{ij}x_ix_j}}$$其中，$x_i$表示第$i$个特征，n表示特征的个数。 由式1可知，组合特征及其参数的的个数为$\\frac{n(n-1)}{2}$个，若特征数据很密集，则可以使用传统方法求解。但若在CTR预估等场景中，进行One-Hot Encoding后，特征会变得非常稀疏(Sparse)，满足$x_ix_j$都不为零的记录很少，使用传统方法求解会带来很大的误差。 观察，由$w_{ij}$组成的矩阵可以表示为 即对应的$w_{ij}=v_iv_j^T$，可以将$v_i$和$v_j$看作是$x_i$和$x_j$自带的辅助向量，具体地$v_i=(v_{i1},v_{i2},…,v_{ik})$，$v_i$的维度$k$反映了模型的表达能力。因此，二阶多项式模型也可以表示为：$$y(x)=w_0 + \\sum_\\limits{i=1}^{n}{w_ix_i} + \\sum_\\limits{i=1}^{n}{\\sum_\\limits{j=i+1}^{n}{\\langle v_i,v_j\\rangle x_ix_j}}$$将组合特征的参数$w_{ij}$转化为$\\langle v_i,v_j\\rangle$，确保了在稀疏的情况下仍然能够求得$w_{ij}$的解，这就是FM的核心思想。 FM的求解过程……（待补充）","link":"/2018/12/19/FM/"},{"title":"Python Selenium 快速上手","text":"selenium是用于浏览器测试的自动化工具，可以直接运行在浏览器中，模拟用户的各种操作。 在爬虫领域的应用，主要就是模拟人的操作，打开浏览器，获取网页源码。获得网页源码之后就可以再利用BeautifulSoup、XPath等去解析和提取我们需要的数据了。 本文主要介绍了Python Selenium的一些基础操作，如打开浏览器、打开网页、网页切换、获取网页源码等操作。 1 导入核心模块selenium.webdriverselenium.webdriver模块是使用中的核心模块，提供了常用浏览器的WebDriver实现。目前支持的WebDriver实现有Firefox，Chrome，IE和Remote等，本文主要使用Chrome。 12# 导入driver模块from selenium import webdriver 2 打开浏览器导入driver模块之后，就可以使用它打开浏览器了，代码如下所示 12browser = webdriver.Chrome(chrome_path) # chrome_path为本地的chrome驱动的路径browser_phantomjs = webdriver.PhantomJS(phantomjs_path) # phantomjs_path为本地的phantomjs_path的路径 3 打开网页打开Chrome浏览器后，会打开一个默认的空白标签页，接下来就可以在标签页中打开一个网页了，代码如下所示： 123url = r\"https://www.jianshu.com/u/a9f6f9178f66\"browser.get(url) # 打开网页init_handle = browser.current_window_handle # 获得当前标签页的句柄(用来定位不同的标签页) 在浏览器中可以打开很多个标签页，每一个标签页都有唯一的标识，也被我们称作标签页的句柄，可以用来在不同的标签页之间切换。 4 打开多个网页我们也可以在新的标签页中打开另一个网页，需要通过执行一个简单的脚本语句实现，脚本语句如下第2行代码所示： 1234new_url = r\"https://github.com/Oythonhill\"open_new_tag_command = \"window.open(\"&#123;&#125;\")\".format(new_url) browser_chrome.execute_script(open_new_tag_command) # 执行脚本语句，在新的标签页中打开网页total_handles = browser.window_handles # 获取所有标签页句柄（列表类型） 同样的，我们也可以获取当前所有打开标签页的句柄（以列表类型呈现）。 当然，我们页可以切换到之前的标签页，代码如下：1browser.swich_to.window(init_handle) # 根据之前获取的标签页句柄定位 5 获取源码获取当前页面的网页源码，获得源码之后就可以通过Beautiful或者XPath进行解析了，当然也可以使用Selenium内置的定位元素的一些方法，这个在另外一篇再说1source_code = browser.page_source 最后，就是关闭标签页或者浏览器12browser.close() # 关闭当前标签页browser.quit() # 关闭整个浏览器","link":"/2018/11/29/Python Selenium 快速上手/"},{"title":"Python 字典简介","text":"刚开始使用Python时，主要是使用列表比较多。慢慢地用Python处理的数据变多、变复杂，特别是经常使用pandas、numpy时，由于字典可以无缝转化成pandas的DataFrame或者JSON格式的数据，可以带来很多便利。 2 字典简介字典是一种可变容器，可以存储任意类型的对象。 字典也是Python中唯一内建的映射（mapping）类型，常见格式如下： 123dict = &#123;&apos;Alice&apos;: &apos;2341&apos;, &apos;Beth&apos;: &apos;9102&apos;, &apos;Cecil&apos;: &apos;3258&apos;&#125; 字典的每个键值对用冒号(:)分割，每个对之间用逗号(,)分割，整个字典包括在花括号({})中，空字典由两个大括号组成 。键必须是唯一的，但值则不必。值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组。 本文主要介绍字典的定义、创建方法以及一些基本操作。 3 创建字典3.1 直观方法直接地，在大括号里输入如下的键值对，即创建了一个字典。 123&gt;&gt;&gt; dict = &#123;'Alice': '2341', 'Beth': '9102', 'Cecil': '3258'&#125; 3.2 dict函数数据处理的时候，使用的更多的是dict函数，通过其他映射或者（键，值）对的序列来创建字典，代码如下： 123456&gt;&gt;&gt; items = [('name', 'Gumby'), ('age','42')]&gt;&gt;&gt; my_dict = dict(items)&gt;&gt;&gt; my_dict&#123;'age': 42, 'name': 'Gumby'&#125;&gt;&gt;&gt; my_dict['name']'Gumby' 或者通过关键字参数来创建字典，代码如下： 123&gt;&gt;&gt; my_dict = dict(name='Gumby', age=42)&gt;&gt;&gt; d&#123;'age': 42, 'name': 'Gumby'&#125; 其他的数据格式也常常能很方便地转化为字典，例如JSON格式的数据，这里不再展开。 4 字典基本操作 操作 注释 dict[key] 返回键key所对应的值 dict[key]=value 将键key所对应的值赋成value del dict[key] 删除字典中键为key的项 key in dict 检查dict中是否含有键为key的项 len(dict) 返回dict中项（键—值对）的数量 在字典中检查键的成员资格比在列表中检查值的成员资格更高效","link":"/2018/12/19/Python 字典简介/"},{"title":"Python 字典方法","text":"继上文”字典简介”后，本文继续介绍字典的常用方法。 1 Clearclear方法可以清除字典里所有的项，这是个原地操作（类似于list.sort），所以无返回值（或者说返回值为None）。示例代码如下： 12345678910&gt;&gt;&gt; my_dict = &#123;&#125;&gt;&gt;&gt; my_dict['name'] = 'Oythonhill'&gt;&gt;&gt; my_dict['age'] = 22&gt;&gt;&gt; my_dict&#123;'age': 22, 'name': 'Oythonhill'&#125;&gt;&gt;&gt; returned_value = my_dict.clear()&gt;&gt;&gt; my_dict&#123;&#125;&gt;&gt;&gt; print(returned_value)None 2 Copycopy方法返回一个具有相同键值对的新字典（浅复制），示例代码如下： 12345&gt;&gt;&gt; x = &#123;'author': 'oythonhill', 'location': 'beijing'&#125;&gt;&gt;&gt; y = x.copy()&gt;&gt;&gt; y['author'] = 'pythonhill'&gt;&gt;&gt; y['location'] = 'shanghai'&gt;&gt;&gt; y mydict.copy() copy方法返回mydict的地址副本(Shallow copy)，如果替换了地址副本中值的地址，新旧dict间不会产生影响，但如果对地址副本中地址对应的值作直接修改，则会同时影响新旧dict，其影响可以用deep copy的方法来避免 3 Fromkeys4 Getmydict.get(key,default=?) get方法可以用来访问给定键对应的值，若给定键不存在，则会返回默认值。 5 setdefaultSetdefault mydict.setdefault(key=,values=None) 若给定键存在，返回给定键对应的值，若给定键不存在，返回默认值，并根据给定键和默认值更新字典。 6 Items和Iteritems返回一个列表，元素为元组，每个元祖包含一个键和该键的值，示例代码如下： 123&gt;&gt;&gt; dict = &#123;'author': 'oythonhill', 'location': 'beijing'&#125;&gt;&gt;&gt; dict.items()[('author','oythonhill'), ('location','beijing')] iteritems方法的作用大致相同，但是会返回一个迭代器对象。 7 Keys和Iterkeyskeys方法返回一个列表，包含字典所有的键； iterkeys方法类似，但返回的是一个迭代器。 8 Values和Itervaluesvlaues方法返回一个列表，包含字典的所有值； itervalues方法类似，返回的是一个迭代器。 9 Pop和Popitem pop方法用来获得对应于给定键的值，然后将这个键值对从字典中删除，示例代码如下： 12345&gt;&gt;&gt; dict = &#123;'x': 1, 'y': '2'&#125;&gt;&gt;&gt; d.pop('x')1&gt;&gt;&gt; dict&#123;'y': '2'&#125; popitem方法会从字典中随机弹出一项（键值对），类似于list.pop，但list.pop是有序的，popitem是随机的，示例代码如下: 12345&gt;&gt;&gt; dict = &#123;'x': 1, 'y': '2', 'z': '3'&#125;&gt;&gt;&gt; dict.popitem()('y', '2')&gt;&gt;&gt; dict&#123;'x': 1, 'z': '3'&#125;","link":"/2018/02/03/python 字典方法/"},{"title":"Spark Streaming基础总结","text":"最近在看Spark的相关东西，主要包括Spark的基础知识、Spark Streaming等，这里作一些总结。 1 Introduction从时间维度可以将数据分析分为历史数据的分析和实时数据的分析，例如Hive可以实现对于历史全量数据的计算，但是花费时间往往较长。实际场景中，如“双11”各大电商平台实时计算当前订单的情况时，需要实时对各个订单的数据依次进行采集、分析处理、存储等步骤，对于数据处理的速度要求很高，而且需要保持一段时期内不间断的运行。对于这类问题，Spark通过Spark Streaming组件提供了支持，Spark Streaming可以实现对于高吞吐量、实时的数据流的处理和分析，支持多样化的数据源如Kafka、Flume、HDFS、Kinesis和Twitter等。 2 DStream Model类似于RDD之于Spark，Spark Streaming也有自己的核心数据对象，称为DStream（Discretized Stream，离散流）。使用Spark Streaming需要基于一些数据源创建DStream，然后在DStream上进行一些数据操作，这里的DStream可以近似地看作是一个比RDD更高一级的数据结构或者是RDD的序列。 虽然Spark Streaming声称是进行流数据处理的大数据系统，但从DStream的名称中就可以看出，它本质上仍然是基于批处理的。DStream将数据流进行分片，转化为多个batch，然后使用Spark Engine对这些batch进行处理和分析。 这里的batch是基于时间间隔来进行分割的，这里的批处理时间间隔（batch interval）需要人为确定，每一个batch的数据对应一个RDD实例。 2.1 Input DStreamInput DStream是一种特殊的DStream，它从外部数据源中获取原始数据流，连接到Spark Streaming中。Input DStream可以接受两种类型的数据输入： （1）基本输入源：文件流、套接字连接和Akka Actor。 （2）高级输入源：Kafka、Flume、Kineis、Twitter等。 基本输入源可以直接应用于StreamingContext API，而对于高级输入源则需要一些额外的依赖包。 由于Input DStream要接受外部的数据输入，因此每个Input DStream（不包括文件流）都会对应一个单一的接收器（Receiver）对象，每个接收器对象都对应接受一个数据流输入。由于接收器需要持续运行，因此会占用分配给Spark Streaming的一个核，如果可用的核数不大于接收器的数量，会导致无法对数据进行其他变换操作。 2.2 DStream TransformationDStream的转换也和RDD的转换类似，即对于数据对象进行变换、计算等操作，但DStream的Transformation中还有一些特殊的操作，如updateStateByKey()、transform()以及各种Window相关的操作。下面列举了一些主要的DStream操作： Transformation Interpretation map(func) 对DStream中的各个元素进行func函数操作，然后返回一个新的DStream. flatMap(func) 与map方法类似，只不过各个输入项可以被输出为零个或多个输出项. filter(func) 过滤出所有函数func返回值为true的DStream元素并返回一个新的DStream. repartition(numPartitions) 增加或减少DStream中的分区数，从而改变DStream的并行度. union(otherStream) 将源DStream和输入参数为otherDStream的元素合并，并返回一个新的DStream. count() 通过对DStream中的各个RDD中的元素进行计数. reduce(func) 对源DStream中的各个RDD中的元素利用func进行聚合操作，然后返回只有一个元素的RDD构成的新的DStream. countByValue() 对于元素类型为K的DStream，返回一个元素为（K,Long）键值对形式的新的DStream，Long对应的值为源DStream中各个RDD的key出现的次数. reduceByKey(func, [numTasks]) 利用func函数对源DStream中的key进行聚合操作，然后返回新的（K，V）对构成的DStream. join(otherStream, [numTasks]) 输入为（K,V)、（K,W）类型的DStream，返回一个新的（K，（V，W））类型的DStream. cogroup(otherStream, [numTasks]) 输入为（K,V)、（K,W）类型的DStream，返回一个新的 (K, Seq[V], Seq[W]) 元组类型的DStream. transform(func) 通过RDD-to-RDD函数作用于DStream中的各个RDD，返回一个新的RDD. updateStateByKey(func) 根据于key的前置状态和key的新值，对key进行更新，返回一个新状态的DStream. Spark Streaming提供了基于窗口（Window）的计算，即可以通过一个滑动窗口，对原始DStream的数据进行转换，得到一个新的DStream。这里涉及到两个参数的设定： （1）窗口长度（window length）：一个窗口覆盖的流数据的时间长度，必须是批处理时间间隔的倍数。窗口长度决定了一个窗口内包含多少个batch的数据。 （2）窗口滑动时间间隔（slide interval）：前一个窗口到后一个窗口所经过的时间长度，必须是批处理时间间隔的倍数。 2.3 DStream Output OperationsDStream的输出操作（Output Operations）可以将DStream的数据输出到外部的数据库或文件系统。与RDD的Action类似，当某个Output Operation被调用时，Spark Streaming程序才会开始真正的计算过程。 下面列举了一些具体的输出操作： Output Operations Interpretation print() 打印到控制台. saveAsTextFiles(prefix, [suffix]) 保存DStream的内容为文本文件，文件名为”prefix-TIME_IN_MS[.suffix]”. saveAsObjectFiles(prefix, [suffix]) 保存DStream的内容为SequenceFile，文件名为 “prefix-TIME_IN_MS[.suffix]”. saveAsHadoopFiles(prefix, [suffix]) 保存DStream的内容为Hadoop文件，文件名为”prefix-TIME_IN_MS[.suffix]”. foreachRDD(func) 对Dstream里面的每个RDD执行func，并将结果保存到外部系统，如保存到RDD文件中或写入数据库. 3 Fault Tolerance容错（Fault Tolerance）指的是一个系统在部分模块出现故障时仍旧能够提供服务的能力。一个分布式数据处理程序要能够长期不间断运行，这就要求计算模型具有很高的容错性。 Spark操作的数据一般存储与类似HDFS这样的文件系统上，这些文件系统本身就有容错能力。但是由于Spark Streaming处理的很多数据是通过网络接收的，即接收到数据的时候没有备份，为了让Spark Streaming程序中的RDD都能够具有和普通RDD一样的容错性，这些数据需要被复制到多个Worker节点的Executor内存中。 Spark Streaming通过检查点（Check Point）的方式来平衡容错能力和代价问题。DStream依赖的RDD是可重复的数据集，每一个RDD从建立之初都记录了每一步的计算过程，如果RDD某个分区由于一些原因数据丢失了，就可以重新执行计算来恢复数据。随着运行时间的增加，数据恢复的代价也会随着计算过程而增加，因此Spark提供了检查点功能，定期记录计算的中间状态，避免数据恢复时的漫长计算过程。Spark Streaming支持两种类型的检查点： （1）元数据检查点。这种检查点可以记录Driver的配置、操作以及未完成的批次，可以让Driver节点在失效重启后可以继续运行。 （2）数据检查点。这种检查点主要用来恢复有状态的转换操作，例如updateStateByKey或者reduceByKeyAndWindow操作，它可以记录数据计算的中间状态，避免在数据恢复时，长依赖链带来的恢复时间的无限增长。 开启检查点功能需要设置一个可靠的文件系统路径来保存检查点信息，代码如下： 1streamingContext.checkpoing(checkPointDirectory) //参数是文件路径 为了让Driver自动重启时能够开启检查点功能，需要对原始StreamingContext进行简单的修改，创建一个检查检查点目录的函数，代码如下： 12345678910111213def functionToCreateContext(): StreamingContext = &#123; val ssc = new StreamingContext(...) ssc.checkpoint(checkpointDirecroty) //设置检查点目录 ... val lines = ssc.socketTextStream(...) //创建DStream&#125;//从检查点目录恢复一个StreamingContext或者创建一个新的val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext())//启动contextcontext.start()context.awaitTermination() [1] 于俊. Spark核心技术与高级应用[M]. 机械工业出版社, 2016. [2] 陈欢林世飞. Spark最佳实践[M].人民邮电出版社, 2016.","link":"/2018/11/28/Spark Streaming基础总结/"}],"tags":[{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"Python爬虫","slug":"Python爬虫","link":"/tags/Python爬虫/"},{"name":"Python数据结构","slug":"Python数据结构","link":"/tags/Python数据结构/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"}],"categories":[{"name":"ML算法","slug":"ML算法","link":"/categories/ML算法/"},{"name":"Python应用","slug":"Python应用","link":"/categories/Python应用/"},{"name":"Python基础","slug":"Python基础","link":"/categories/Python基础/"},{"name":"Python爬虫","slug":"Python应用/Python爬虫","link":"/categories/Python应用/Python爬虫/"},{"name":"Spark基础","slug":"Spark基础","link":"/categories/Spark基础/"}]}